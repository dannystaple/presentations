*title* One of the things I’m interested in, inasmuch as ‘big data’ is a thing in history, is the way our tool use changes us. While I’ve got ‘digital humanities’ in my job description, I come to it by way of archaeology, and I take it as an article of faith that our tools change us as much as we change our tools. We co-respond with our tools in the making of things. But I guess I worry that our tools are getting out of control, that we exult in the ways our tools exceed us. 

1. In recent years, everyone has suddenly had to cope with the sheer amount of data that our always-on, always-connected, always-under-surveillance society has generated. It’s forced us to take stock, and rethink our place in the world. This kind of thing has happened before, when change has happened so rapidly we don't quite know what to do; in an earlier moment it gave rise to ‘the Gothic’ – and it’s worthwhile thinking through *that* reaction to the changing place of humans in the world, and what it implies for *this* moment. To simplify horribly, and at the risk of undermining my humanities street-cred, I will for the sake of convenience conflate romanticism with the gothic and boil the gothic down to the text on this slide; this is all tied up the broader changes in western society precipitating out of the enlightenment and the beginnings of our industrial age. If I said ‘Frankenstein, or the modern prometheus’, you get what I’m talking about. Key here is the idea of shock and thrill …

2. …and the annihilation of ‘the self’. That is, the real ambition here with the gothic is to first frame everything from the point of view of the individual, to overwhelm the senses of the viewer or the reader in the terror or majesty of, say, a landscape or a sensation (the gothic horror), such that only the sublime feeling remains. The quantified self, and those people who keep fit-bits and personal trackers, would be at home in the gothic – and indeed, it is from an event explicitly tying that aspect of ‘personal’ big data to the gothic sensibility (see this; also, read this) that I began thinking along these lines. But let’s return to that idea of the annihilation of ‘self’.

3. It’s in this sense that big data is gothic. It absolves us of the burden of individual identity. The traces that we leave are aggregated and interrogated and correlated, a vast digital landscape of signs and signals that only the machine can adequately contemplate, from which correlations are extracted and predictions are generated. Here's the heat map that did the rounds a few weeks ago, showing aggregated visitor traffic in the national gallery, from surveillance of visitor's cellphone wifi signals. It's a map to warm the heart of anyone who has to plan visitor flow. Gallery visitors, in aggregate, are a nusiance; and there's no time to think about what that one dot there felt when she was looking at the painting, the way it moved her, the way it reminded her of holidays with her family when she was a child. No, we can agree that perhaps this is an ok sublimation of the self into the wider digital landscape, for the wider digital good.

3.1 But it's but short steps from monitoring more-or-less anonymous wifi noise to installing museum panels that watch the visitor back. When I was an undergraduate, periodically paniced professors would send our class into the library to pull the little-used but good-to-have volumes off the shelves, and leave them on the carells, so that the Librarians would not axe them. Imagine a musem panel equipped with visitor tracking cameras, monitoring attention spent looking at a painting. Not enough time spent? Well why is this on the wall? Pitch it!

3.2 This technology currently exists. It's watching you on the high street right now. A picture posted to reddit a few weeks ago caught a pizza parlour's menu screens - you can see that the main display has crashed, but the camera logs and their identifications of patrons continues unabated. Cross reference this information with debit or credit card information from the till, and we can begin to see where 'surveillance capitalism' is going to go. Do we want surveillance cultural heritage? Are we ready for that? 
https://medium.com/@ys/attention-dubliners-we-are-all-being-spied-on-by-a-little-known-company-on-camden-street-5c784558dc4e
https://www.kairos.com/blog/anonymous-video-analytics-ava-technology-privacy
https://thecreativeconversation.wordpress.com/2017/05/02/museums-are-tracking-visitors-through-their-wifi-to-see-whats-popular/  

3.3 Without our knowledge, without our consent, we are all living inside a vast experiment that not even its framers fully understand. Microsoft can build a chat-bot that learns from humans, but they don’t understand or can’t foresee that releasing it into a particular environment already toxic for women - Twitter - is going to be a bad idea. The terror and majesty of the algorithm, of the code, of the data, of the tech – never mind the humans in the foreground – is what matters. 

3.4 I mean: even good bots fight, as the recent paper by x, y, z has it. That is, simple wikipedia editing bots that are meant to ease the burden of fixing citations, finding broken links, and so on, could get locked in endless cycles of reverting each other's changes because they were each conceived of in isolation: the broader context in which they were working was not considered (ie, that something else might come and 'fix' what it'd just 'fixed'). Now apparently, Wikipedia has made changes that make these bot wars much less likely now. But I've seen bots of my own that I've released into Twitter get locked into a vicious cycle of smacking each other down. And it's only because these bots are so patently absurd that no actual human got involved, I think. I had to kill them. I built them because I could; not because I had anything meaningful to say.  

4. This is what I mean by big data gothic: as Zoe Quin said, ‘if you’re not asking yourself ‘how could this be used to hurt someone’ in your design/engineering process, you’ve failed’. That is to say: you’ve been seduced by the data vista stretching out before you. This is the same impulse that Steve Jobs channeled when he (more or less) said ‘I don’t do marketing, I do what I want because people don’t know what they want’. It’s the same impulse that reaches for plagiarism detection software rather than asking, ‘what is it about my course that makes plagiarism a rationale response’. It's the same impulse that gives us 'ProctorU' to monitor students in online courses. It's the same impulse that makes us reach for surveillance software to understand how people move through space rather than employing say something like space-syntax coupled with human (visible) observers. The seductive lure of ‘moar data, cheaper data’ suggests that eventually, all solutions will percolate out of the data. But who decides what *counts* as data?

5. It’s the rhetorical and tactical usage of the phrase ‘big data’ that I’m concerned with here; I’m not against data science per se or the interesting things you can learn when you have aggregated information – I am an archaeologist after all, and I *did* publish a book with ‘big data’ in the title. The thing is, what we call things matters. It is not for nothing that Hyacinth Bucket insisted that it was pronounced Bouquet. Metaphors structure thought and action – ‘the university is a business’, for instance – and so if we imagine ‘big data’ as somehow objectively out there, and not produced by conscious decisions about *what counts*, and *who does the counting*, we end up in situations where people lose their jobs (uber!) or miss out on credit, or constantly get reconnected with abusive ex partners on facebook, or, I don't know, billionaires bank rolling disinformation campaigns. Big data, as an aggregated thing, means that the means of production, the power in the system, has shifted from those of us who create the data, to those of us with the money, the privilege, the computing power, to mine it.

5.5 Big data is about the power that comes from information assymetry. 

6. So let’s call this blinkered version of working with human lives ‘Big Data Gothic’. Like its namesake, it’s not too concerned about fallout on individual, named, humans; it revels in the data landscape and draws much of its power from the thrill of off-loading decision making to the machines…

7. That is to say, it begins by thinking of people as things.

8. Oddly enough, big data - as I've described it here today - is not concerned with context; but – again, as an archaeologist – context forces us to think of humans as, well, human.

9. This is what the humanities excel at. <- delete this?

10. When you think of humans as things, a poorly trained machine learning routine can be used to target other humans for killing. That the routine has a probable error rate that translates into 15 000 people slated for death, people shrug. Much easier to say, there’s a 0.008% false positive rate. (see http://arstechnica.co.uk/security/2016/02/the-nsas-skynet-program-may-be-killing-thousands-of-innocent-people/)

11. Treating humans as things. Is there anything more thing-like, than buying and selling human remains? The literal commodification of humans. This is a project I’m working on with Damien Huffer; he does the anthropology, I do the numbers. Human remains are bought and sold on Instagram in ways that circumvent Instagram’s rules (such as they are). We want to know both the language used to facilitate these sales, and also the visual language, that isn’t caught by my algorithmic trawling. I have around, I donno, perhaps 15000 images and posts now on my machine. ‘Big’ in this context means: overwhelming for one person using the tools he was taught in grad school. Big data from culling the posts gives me some insight, esp when I represent as vector models, some of the explicit language behind this trade, and ways that people signal that something is for sale. 

11.1 Here the information assymetry runs the other way. Here, there is a community that coalesces, reforms, and reconfigures around images of the dead. I am an outsider. I use the tools of text analysis and data mining to try to pry my way in. I can map the language of posts to explore sentiment ; we can explore the patterning of words in posts as a kind of geographical space by defining vectors - which reveals that something clearly labelled as 'not for sale' may in fact be available after all, more like 'for sale at a price I won't name publicly'. If we take two such vectors, we can see the positive and negative connotations of different words across the for sale - not for sale binary - craftiness and transformation is much valued. And of course, we can map patterns of connectivity amongst followed-follower relations - here, 26 users that account for a large volume of the trade, connected by followers in common point to subgroups that we are calling enthusiasts, generalists, and specialists. This is all currently under review for publication, so I won't go too much further into these results.

11.1 But this approach, for all that it is fascinating, also misses the visual signals in the composition of the images itself. To address the visual means that I end up using many of the same computer vision APIs and tools that I was upset about earlier. These hidden cues that I am trying to understand, their usage is rather like a kind of steganography that is explicitly meant to conceal the trade from algorithmic monitoring. By the way, this kind of reaction - pushing back against the algorithms- is also present on Facebook or Twitter as people ‘template’ themselves for particular audiences. The danger is that these templated selves could become algorithmic prisons: our performances in reaction to alogorithms that make assumptions about how the world work cease to become performances and instead become real. This is big data gothic.

12. Tricia Wang prefers the term ‘thick data’, that is, the kind of thick storytelling that ethnography, anthropology, history, english, and so on, excel at. She argues with reference to what happened to Nokia – and I agree with her – that insight is dependent on both modes; that data science can usefully learn a thing or two from the humanities, and likewise, the humanities can benefit from the scale and distance that an aggregated view can provide. OpenContext. DiNAA, tDAR.

12.1 insert here a digression of some of the interesting ways that this is happening. Bevan paper re PAS. My own topic model re PAS data (and discuss how *awful* these visualizations are). Recent piece in IA. topic model of articles. topic model of inscriptions. discourses in PAS modeled against roman regions. discourses in original context sheets unpeels the official objectivity of the excavation 

David Mimno in 2009 turned the tools of data analysis on the databases of household materials recovered and recorded room by room at Pompeii. He considered each room as a ‘document’ and the artefacts therein as the ‘tokens’ or ‘words’ within that document, for the purposes of topic modeling. The resulting ‘topics’ of this analysis are what he calls ‘vocabularies’ of object types which when taken together can suggest the mixture of functions particular rooms may have had in Pompeii. He writes, ‘the purpose of this tool is not to show that topic modeling is the best tool for archaeological investigation, but that it is an appropriate tool that can provide a complement to human analysis….mathematically concrete in its biases’.

12.2 how do we not become complicity in data asymetry? use open methods, open source tools, document workflows, no proprietary stuff unless we can't avoid it, document what worked and what didn't - keep a fail log! daylight is the best antidote to big data gothic. Also: screw with expectations and conventions. don't be trapped in doing what we've done before, b/c at scale & with computing power, the unintended consequences can trap us.

12.2 How can we use computing power to break us out of the big data gothic? Maybe sonification - listen to watling street for a bit. Maybe by inventing new tools: play sonic clouds. Reference Lutz's riding the horseless carriage article: our ability to create compelling visualizations that make us understand something new about the data is still stuck in a print, visual-first mode. We need big data DADA. Digital synesthesia. Feel the data. Hear the data. Smell and taste it. Ref Hannah-from-croatia sound work on story arcs in novels: what would the equivalent look like in British Museum? Return humans to the priority.

13. The thing is not to be seduced by the view that big data gothic provides. It is exhilerating, I agree. But it’s ultimately getting in the way. Turn it on its head. Make big DADA. digital creativity and non-conformity. don't replicate previous eras of colonialism and imperialism, b/c if it was bad the first time, it's going to be even worse this time because it's personalized, hidden, one to one, gaslit. Which makes it even harder to organize.

14. I’ll just leave this here: a good way to know if you’re dealing with a big data gothic situation is if you’ve blamed the algorithm. If you’ve offloaded responsibility for the consequences of decisions made to the computer. In the end, it all comes down to humans.